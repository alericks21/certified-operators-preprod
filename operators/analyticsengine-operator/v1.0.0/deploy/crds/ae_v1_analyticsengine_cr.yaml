apiVersion: ae.cpd.ibm.com/v1
kind: AnalyticsEngine
metadata:
  name: analyticsengine-sample
  labels:
    app.kubernetes.io/instance: ibm-cpd-ae-operator
    app.kubernetes.io/managed-by: ibm-cpd-ae-operator
    app.kubernetes.io/name: ibm-cpd-ae-operator
    build: BUILD_NUMBER
spec:
#  version: "4.0.1"
  license:
    accept: true
    license: Enterprise
#-------------------------------------------------------#
# Following spec is mandatory to perform upgrade 
# from 3.5.x to 4.0.x 
#-------------------------------------------------------#
#    is_35_upgrade: false
#-------------------------------------------------------#
# Following specs are optional and
# we have added default values as example in this CR.
#-------------------------------------------------------#
  scaleConfig: "small"
#  pullPrefix: "cp.icr.io/cp/cpd"
#-------------------------------------------------------#
# Following specs are optional and 
# can be altered to change service level configurations
#-------------------------------------------------------#
#  serviceConfig:
#    sparkAdvEnabled: true                         # This flag will enable or disable job UI capabilities of AnalyticsEngine service
#    jobAutoDeleteEnabled: true                    # Set it to false if you do not want to removed spark jobs once they have reached terminal states e.g FINISHED/FAILED
#    fipsEnabled: false                            # Set it to true if your system is FIPS enabled
#    kernelCullTime: 30                            # Change value in minutes, if want to remove idle kernel after X minutes.
#    imagePullCompletions: 20                      # If you have very large Openshift cluster in that case you can update imagePullCompletions & imagePullParallelism accordingly
#    imagePullParallelism: 40                      # e.g. if you have 100 nodes cluster set imagePullCompletions: "100" & imagePullParallelism: "150"
#    kernelCleanupSchedule: "*/30 * * * *"         # By default kernel & job cleanup cronjobs look for idle spark kernels/jobs and based on kernelCullTime parameter
#    jobCleanupSchedule: "*/30 * * * *"            # and removes them, if you want cleanup to less/more aggressive change accordingly. e.g for 1 hour "* */1 * * *" k8s format
#-------------------------------------------------------#
# Following specs are optional and can be altered
# to change spark runtime level configurations 
#-------------------------------------------------------#   
#  sparkRuntimeConfig:                             # If you want to create spark jobs with
#    maxDriverCpuCores: 5                          # Drive CPUs more than 5 than set accordingly  
#    maxExecutorCpuCores: 5                        # or more than 5 CPU per Executor than set accordingly  
#    maxDriverMemory: "50g"                        # or Drive Memory more than 50g than set accordingly  
#    maxExecutorMemory: "50g"                      # or more than 50g Memory per Executor than set accordingly  
#    maxNumWorkers: 50                             # or more than 50 workers/executors than set accordingly  
#-------------------------------------------------------#
# Following specs are optional and can be altered
# to change service instance level configurations.
# Each AE service service instance have resource quota
# (cpu/memory) set by default as following. It can be changed
# via API for an instance but to change default values
# for any new instance creation update following 
#-------------------------------------------------------#
#  serviceInstanceConfig:
#    defaultCpuQuota: 20                           # defaultCpuQuota means accumulative cpu consumption of spark jobs create under an instance can be no more than 20
#    defaultMemoryQuota: 80                        # defaultMemoryQuota means accumulative memory consumption of spark jobs create under an instance can be no more than 80 in GB 
