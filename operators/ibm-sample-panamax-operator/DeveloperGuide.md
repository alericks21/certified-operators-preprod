## Panamax Operator Developer Guide

The Panamax Operator developer guide is a living document about developing this Operator. The goal of this document is to guide developers through the process of developing and building the Panamax Operator. 

This is NOT a general "how to build an Operator" guide. While the document will touch on general Operator-related topics, it hopes to build on Red Hat's [Golang Operator Tutorial](https://sdk.operatorframework.io/docs/building-operators/golang/tutorial/).

See the [Operator Developer Guide](https://playbook.cloudpaklab.ibm.com/operators-getting-started/) in the Cloud Pak Playbook for everything from an introduction to Operators, standards and guidance, and OLM integration.

### Panamax Operator

The Panamax Operator is a Golang-based Operator built using Operator-SDK version 1.3. The 1.0 version of Operator-SDK represented the alignment of Operator-SDK with Kubebuilder, resulting in a new folder structure and moving to using Kustomize as its preferred YAML customization method.

#### Custom Resource Definitions (CRDs)

The Panamax Operator manages two custom resource definitions (CRDs):

- `Panamax`
  - The Panamax custom resource deploys a simple Nginx container that serves up data from a Kubernetes ConfigMap populated by data provided in the custom resource spec.
- `PanamaxConfig`
  - The PanamaxConfig custom resource provides a custom resource for listing all of the supported versions of a Panamax custom resource. When a version is specified in the Panamax custom resource, it uses the PanamaxConfig custom resource to validate that the specified version is valid.

The CRDs are generated in the `config/crd/bases` folder using the `*_types.go` files under the `api/v1beta1` folder. See [Define the API](https://sdk.operatorframework.io/docs/building-operators/golang/tutorial/#define-the-api) in the Red Hat tutorial for more information on how this works.

#### Controllers

The controllers that manage the two custom resources are found in the `controllers` folder. Each controller is registered in `main.go` which is also known as the Manager. Controllers implement the reconcile function, which is called each time the custom resource is created/updated/deleted. It is the reconcile code that determines what action is being performed, what to do about that action, and updating the status of the custom resource based on that action.

See the playbook documentation around recommended practices for [reconciling and requeuing](https://playbook.cloudpaklab.ibm.com/create-controller/#Reconcile) and the Red Hat documentation for more information on [implementing a controller](https://sdk.operatorframework.io/docs/building-operators/golang/tutorial/#implement-the-controller).

#### Role-Based Asset Control (RBAC)

The resources managed by this Operator are defined using the `+kubebuilder:rbac` annotations in your Go code. These annotations generate `config/rbac/role.yaml` when the `make manifests` target is run. See the [Red Hat documentation](https://sdk.operatorframework.io/docs/building-operators/golang/tutorial/#specify-permissions-and-generate-rbac-manifests) for more information.

#### Operator Lifecycle Manager (OLM)

See the [OLM documentation](https://playbook.cloudpaklab.ibm.com/using-the-operator-lifecycle-manager-framework/) in the Cloud Pak Playbook for the most up to date information on OLM-enabling your Operator.

#### Bundle directory

The `bundle/` directory is created by running the `make bundle` command. The `bundle/manifests` directory contains a CSV and all of the CRDs from `config/crds`. These resources are used by the `make bundle-build` command to produce your Operator bundle image. Once pushed to an open registry such as `docker.io` or `quay.io`, this bundle can either be tested on an OCP environment using the `operator-sdk run bundle` command or added to a catalog image using `opm registry add`

See Red Hat's [Creating a bundle](https://sdk.operatorframework.io/docs/olm-integration/quickstart-bundle/#creating-a-bundle) documentation or the playbook's [Tooling overview](https://playbook.cloudpaklab.ibm.com/catalog-tooling-overview/) page for more information.

#### Deploy directory

You'll notice that the Panamax Operator has a deploy directory similar to an Operator-SDK 0.x project. This is something that is built for ease of use, but is not expected in normal projects.

### Building and testing

#### Makefile

The Panamax Operator Makefile the standard targets built when the Operator project was generated by Operator-SDK. See Red Hat's [Golang Operator Tutorial](https://sdk.operatorframework.io/docs/building-operators/golang/tutorial/) for a description of how to use a number of those. This section will discuss the additional targets added to this project and modifications made to Makefile targets.

- `buildPostProcessor` (new)
  - The `buildPostProcessor` target builds the golang binary that is used by `bundle` and `produceNativeKubeResources` to fix things in the produced yaml files that cannot be autogenerated. See [post-processor.go](https://github.ibm.com/CloudPakOpenContent/ibm-sample-panamax-operator/blob/master/cmd/post-processor/post-processor.go#L16) for a list of current updates that this script provides
- `bundle` (updated)
  - The `bundle` target has been updated with a call to `post-processor` to update the resources created by the bundle action.
- `docker-multiarch-build` (new)
  - The `docker-multiarch-build` target builds and pushes images for multiple architectures and a manifest list for them.
- `produceKubeResources` (new)
  - Creates the Operator-SDK 0.x style deploy directory yaml files. This is not something that we would expect to be included in standard Operator-SDK 1.x projects
- `produceNativeKubeResources` (new)
  - This is similar to `produceKubeResources` but adds on the call to `post-processor` and disables webhooks on the CRD.
- `publish-catalog` (new)
  - The `publish-catalog` target first loops through git releases (as well as the volatile version from the current branch) and builds/pushes the bundle image for each release, while building versions.txt as it works. Then, it uses both of those to build and push catalog images on remote VMs, as well as a manifest list for that catalog.
- `update-related-digests` (new)
  - Attempts to lookup and update digests for images listed in cmd/post-processor/relatedImages.csv. See [Using update-related-digests](#using-update-related-digests) for more information

#### Testing natively

The `produceNativeKubeResources` make target will create a single yaml file called `deploy/manifestNativeDeploy.yaml` that contains all of the Kubernetes resources needed to deploy your Operator's controller. This can be used to verify that the output of kustomize is producing the expected resource yaml and that the RBAC annotations have been set up correctly. Once that has been verified, log onto an OCP cluster and use `oc apply -f deploy/manifestNativeDeploy.yaml` to create the resources on your OCP cluster.

#### Testing your Operator bundle

Steps to test your catalog bundle using the Makefile:

1. `make buildPostProcessor` to build the post-processor Go binary
2. `make bundle` to generate the manifests and metadata for the bundle
3. `make bundle-build` to build the bundle image
    - The default bundle image name can be updated with the `BUNDLE_IMG` environment variable. See the Makefile for more information
4. Push the bundle to a personal repo that does not require authentication to pull (Quay is preferred)
5. Log into an OCP instance and create a project/namespace for your deploy. Use `oc project [ns]` to point your CLI to use that namespace
6. `operator-sdk run bundle [bundle_image]` to dynamically create a catalogsource and deploy your Operator from the bundle into your current namespace

#### Creating a catalog

Once your catalog has been successfully tested, the next step is to create an OLM Catalog Index Image. See the [Creating a Catalog Index Image](https://playbook.cloudpaklab.ibm.com/olm-creating-a-catalog-index-image/) playbook page for instructions on how to do this.

The following steps are for the example panamax implementation of multiarch catalog index images (utilizing the git submodule [operator-build-scripts repo](https://github.ibm.com/CloudPakOpenContent/operator-build-scripts) to do the heavy lifting):

1. `make docker-multiarch-build` to create the multiarch version of the operator image

1. Manually change the `image` field in the CSV and `manifestsNativeDeploy.yaml` to point at that digest

1. `make publish-catalog` to create the multiarch operator catalog (for ocp v4.5 and v4.6)

**Note:** To build for a multiarch environment with these scripts, you must set up worker nodes. For information on how to do that, see [operator-build-scripts README](https://github.ibm.com/CloudPakOpenContent/operator-build-scripts/blob/master/README.md)

#### Using `update-related-digests`

The `update-related-digests` target will read each image listed cmd/post-processor/relatedImages.csv and attempt to determine the image's current digest. If the digest has changed, it will update the entry in the file. It does this by using the image path and the image tag specified in cmd/post-processor/relatedImages.csv to look up the entry in the specified image registry. You will need to specify four properties in your CSV to be able to look up digests:

1. Name - the name of the image to be used in `spec.relatedImages`
2. Image - the full path to the image with digest
3. Tag - the image tag
4. Arch - the image architecture (`amd64`, `ppc64le`, `s390x`) or `list` for a manifest list image

For example,

```raw
RELATED_IMAGE_PANAMAX,cp.icr.io/cp/ibm-panamax-nginx@sha256:de4bf2ffa71e255bfd4ec9a237f6c95cd894fa82d315840dca65d98a6de9cd30,v3.0.13,list
RELATED_IMAGE_PANAMD64,cp.icr.io/cp/ibm-panamax-nginx@sha256:123456789010abcdef101112131415161718191a1b1c1d1e1f20212223242526,v3.0.13-amd64,amd64
```

**Specifying credentials and alternative repositories**

The images listed in relatedImages.csv should all be Operand images, which means that they exist in an image registry that requires authentication. The code is written such that it can use the credentials in your local docker config.json for registries such as artifactory, but the entitled registry and staging registry require environment variables.

Additionally, since you will be using `update-related-digests` prior to releasing your product, it supports using the `registries.conf` format for redirecting image paths. Use the following environment variables to take advantage of these enhancements:

- `DOCKER_AUTH_FILE` the location of the docker config.json on your file system (default "$HOME/.docker/config.json")
- `REGISTRIES_CONFIG_PATH` the location of the registries.conf on your file system for mirroring images (default "first $HOME/.config/containers/registries.conf, then /etc/containers/registries.conf")
- `CP_USER` the entitled registry user name (default "")
- `CP_APIKEY` the entitled registry api key (default "")
- `CP_APIKEY_STG` the staging entitled registry api key (default "")

**Note:** Due to a limitation with the library we are using, if you want to use `cp.icr.io` as your image registry, you need to redirect using `registries.conf` - i.e. you need to redirect `cp.icr.io` back to itself (`cp.icr.io`) to tell the code to use the `CP_USER` and `CP_APIKEY` environment variables.